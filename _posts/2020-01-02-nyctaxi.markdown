---
layout: post
title:  "A Look at the NYC Yellow Cab Dataset"
date:   2020-01-02 00:00:00
categories: example
---

<p align="center"><img src="{{ site.url }}/assets/nyc_heatmap.png" height="600"> </p>

### Introduction

### The Dataset

For the file "yellow_tripdata_2016-06.csv" columns and min/max values looks like this
```
Columns:
    dropoff_datetime       datetime  [1996-06-20 16:23:24, 2016-07-01 23:55:31]
    dropoff_latitude       float32   [     0.000000,     60.040714]
    dropoff_longitude      float32   [-118.18625641,  106.24687958]
    fare_amount            float32   [   -450.00000,  628544.75000]
    improvement_surcharge  float32   [ -0.300000012,  11.640000343]
    mta_tax                float32   [ -2.700000048,  60.349998474]
    passenger_count        int32     [            0,             9]
    payment_type           unicode
    pickup_datetime        datetime  [2016-06-01 00:00:00, 2016-06-30 23:59:59]
    pickup_latitude        float32   [     0.000000,     64.096481]
    pickup_longitude       float32   [-118.18625641,    0.00000000]
    rate_code              int32     [            1,            99]
    store_and_fwd_flag     unicode
    surcharge              float32   [ -41.22999954,  597.91998291]
    tip_amount             float32   [ -67.69999695,  854.84997559]
    tolls_amount           float32   [ -12.50000000,  970.00000000]
    total_amount           float32   [   -450.79999,  629033.75000]
    trip_distance          float32   [     0.000000,  71732.703125]
    vendor_id              unicode
```

An initial question was if the data is different depending on the
`vendor_id` column.  This column has the following distribution
```
  vendor        #rows
       1    102008367
       2    113511124
     CMT    505603754
     DDS     14169148
     VTS    513853077
```


### Locational Data

Here are heatmaps of cab dropoff positions around central Manhattan.
The most common positions are the the the Penn Station exits on 7th
and 8th Avenue.  Dropoff positions are more distributed over the area,
we'll have a quick look at that later.  We plot one heatmap for each
`vendor_id`.

<p float="center">
   <img src="{{ site.url }}/assets/coord_1_D.png" height="400"/>
   <img src="{{ site.url }}/assets/coord_2_D.png" height="400"/>
</p>
<p float="center">
   <img src="{{ site.url }}/assets/coord_CMT_D.png" height="400"/>
   <img src="{{ site.url }}/assets/coord_DDS_D.png" height="400"/>
</p>
<p float="center">
   <img src="{{ site.url }}/assets/coord_VTS_D.png" height="400"/>
   <img src="{{ site.url }}/assets/coord_VTSflip_D.png" height="400"/>
</p>

Before jumping to conclusions, we note that the plots cover _all_ data
for each vendor, and the most noisy graphs corresponds to the vendors
with most data.  However, limiting each graph to only 100.000.000
points, the trend is similar.  We have not investigated this further.

The bottom right image is interesting.  This shows data from the "VTS"
vendor plotted at the position on the _South Pole_ that corresponds to
the location of Empire State Building with its latitude and longitude
coordinates swapped!  Some equipment out there has been travelling
with its coordinates mixed up.  Interesting!  There are probably more
issues like this, see section on data quality below.






### Number of Ongoing Taxi Rides per Time Instance

Each row in the dataset corresponds to a complete trip.  A row has
both starting and ending timestamps.  The time resolution is one
second.  To compute the number of ongoing trips, it is more than
sufficient to work on a minute resolution.  There are about four
million minutes in the dataset (2009-01-01 to 2016-06-30), which is a
reasonable amount of data to handle without difficulty.

If we want to compute a graph of the number of ongoing trips per
minute, we can proceed as follows:

1.  Create a vector of all zeros with a length four million.  Each
    item corresponds to one minute

2.  For each ride, increase the vector item at the start time offset by one,
    and decrease the vector item at the stop time offset by one

3.  Integrate the vector to get the number of ongoing trips per minute

Let's look at the resulting curves.


#### Yearly Cycles

We start with a higher view of all the data from 2009 to 2016.
Plotting with a minute resolution is far to noisy to be useful when
looking at this scale.  In fact, there are thousands of samples for
each pixel on screen.  So we downsample.  Actually, we do downsampling
in two steps.  First, all data is added into bins, where each bin
representing one calendar day.  From there, a 30 day root raised
cosine filter is applied to smooth the curve into a month-like
timescale.  The resulting curve is shown below:

<p align="center"><img src="{{ site.url }}/assets/nyc_rides_yearly.png" height="200"> </p>

There is a clear periodicity, with a period time of one year.  For
example, there is a strong dip during the summer, a narrower one
during Christmas, and the peak is somewhere around April.


#### Daily and Weekly Cycles

In order to make sense of a plot with minute resolution, we have to
zoom in.  This is preferably done interactively with some plotting
tool.  Since the datafile is about 94MB, one has to choose this
program with care.  We use [gnuplot](http://www.gnuplot.info/), and it
handles this file pretty well.

Here is one interesting zoom-in, covering December 2015 and January
2016:

<p align="center"><img src="{{ site.url }}/assets/nyc_rides_201512-201601.png" height="350"> </p>

Here we can see the daily and weekly periods.  We can also see that
there is a drop in the number of simultaneous taxi rides over
Christmas.  But what is the drop all the way down to zero that we see
in late January?  Were there no cabs on duty at that time?

We zoom in:

<p float="center">
   <img src="{{ site.url }}/assets/nyc_rides_snowzilla.png" height="350"/>
   <img src="{{ site.url }}/assets/nyc_wikipedia_snowzilla.jpg" height="350"/>
</p>

Some googling reveals that what we see is the [January 2016 United
States Blizzard](https://en.wikipedia.org/wiki/January_2016_United_States_blizzard),
where a travel ban was instituted for New York City.  (Image from Wikipedia.)


#### Comments

Data relating to human activities typically experience periodicity on
year, week, and day levels.  There is a lot of interesting things to
note if playing with the minute resolution file.

According to wikipedia, there are about 15000 "medallions" for New
York City.  Clearly, from the graph above, we can se that it happens
that more than 8000 cabs are in duty at the same time!

Gnuplot has been around and in constant development since 1986.






### The Upside Down





### Try it out Yourself!

#### Download the dataset
#### Install the Accelerator
#### Import the dataset
#### Run the examples


### Dataset Problems

#### Column names

Each data file has a header with column names for all columns in the
file.  Unfortunately, *both names and formatting change between files*.
Some headers use space+comma as the field separator, while some use
just comma.  All data is comma separated, though.  In addition, in
some files there is an empty line between the header and the data.

The solution to these problems is composed of two parts.  First, we
write a function that reads the first lines in the datafile to
determine the labels and how many blank lines there are before the
actual data.  Second, we create a simple dictionary that maps all
label names to a canonical set of names that is used throughout the
dataset.


#### Bad lines

Two of the files in the dataset contain a total of 6809 "unparseable"
lines.  The files are

```
yellow_tripdata_2010-02.csv      # 1126 bad lines
yellow_tripdata_2010-03.csv      # 5683 bad lines
```

All these lines share the property that they contain one more column
than all other lines.

The Accelerator finds and isolates these lines automatically.  This is
the way it works.  On a first data import, the import program will
crash, since parsing is by default assumed to be strict.  Thus, the
programmer is informed there is a problem.  The next step is to re-run
the import at the position of the crash with the `allow_bad` flag set.
When the `allow_bad` flag is set, all unparseable lines are stored in
a separate dataset named `bad`, one for each input file, and after the
full import completes we can check the contents of these datasets.
The simplest way to examine these dataset is using the `dsinfo` tool,
that actually comes with an option to show only those datasets that
contain one line or more of data.  So we run
```
% ax dsinfo nyc-179/bad -C
```
and get
```
    ...
    Full chain length 90, from nyc-0/bad to nyc-178/bad
    Filtered chain length 2
          0: nyc-26/bad (1,126)
          1: nyc-28/bad (5,683)
    6,809 total lines in chain
```

By doing `dsinfo` lookups on the two datasets, we see that they
correspond to the files mentioned above.  Since the datasets are
small, we can investigate them further will simple shell commands, for
example like this

```
% ax dsgrep . nyc-26/bad data | awk -F, '{print NF}' | uniq
```

The command above will grep anything (a dot regexp) in the `data`
column of the `nyc-26/bad` dataset, and paste it to an `awk` program
that will print the number of fields separated by a comma.  This data
is then fed to `uniq` that will print unique lines only once.  The
output of the above program is

```
19
```

while

```
% ax dsinfo nyc-26/default
```

tells us that there are 18 columns in the dataset:
```
    Filename: /zbd/data/nyctaxi2019/yellow_tripdata_2010-02.csv
    ...
    18 columns
    ...
```
The same goes for the dataset `nyc-28/bad`.



#### Outliers


For the complete dataset, min/max values look like this

```
% ax dsinfo nyc-179

Columns:                                 min value       max value
       dropoff_datetime       datetime  [1900-01-01 00:00:00, 2253-08-23 07:56:38]
       dropoff_latitude       float32   [-3579.1394043,  3577.1350098]
       dropoff_longitude      float32   [-3579.1394043,  3460.4267578]
       fare_amount            float32   [-21474808.000,    825998.625]
       improvement_surcharge  float32   [  -0.30000001,  137.63000488]
       mta_tax                float32   [   -3.0000000,  1311.2199707]
       passenger_count        int32     [            0,           255]
       payment_type           unicode   
       pickup_datetime        datetime  [2009-01-01 00:00:00, 2016-06-30 23:59:59]
       pickup_latitude        float32   [-3579.1394043,  3577.1354980]
       pickup_longitude       float32   [-3509.0151367,  3570.2241211]
       rate_code              int32     [            0,           252]
       store_and_fwd_flag     unicode   
       surcharge              float32   [ -79.00000000,  999.98999023]
       tip_amount             float32   [-1677720.1250,  3950588.7500]
       tolls_amount           float32   [-21474836.000,      5510.070]
       total_amount           float32   [-21474830.000,   3950611.500]
       trip_distance          float32   [ -40840124.00,  198623008.00]
       vendor_id              unicode   
```

Clearly, most min and max values does not make any sense: 255
passengers?  3460 degrees (i.e. ten turns around the earth)?
$3950611.5 for a taxi ride?  One of the columns seems to be perfectly
fine, though, and that is the `pickup_datetime` column.  Its range is
exactly as expected.  A bit strange.

A deeper analysis is needed to understand this, of course depending on
the application.  As we've seen there are also correlations between
"vendors" and specific data problems, so not all problems are general
problems.


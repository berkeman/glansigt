---
layout: post
title:  "Fast Processing of the NYC Yellow Cab Dataset"
date:   2020-01-02 00:00:00
categories: example
---

<p align="center"><img src="{{ site.url }}/assets/floff.png" height="800"> </p>




# Introduction

The Accelerator from eBay is designed for fast processing of large
datasets on a single machine.  Programs are simply written in Python,
and thanks to its fast parallel processing and clever reuse of
pre-computed results, execution times are typically in the range of a
few seconds per task.  Therefore, ideas could be tried out with very
little overhead, making the Accelerator a perfect tool for fast
exploration of large datasets.

In this post, we look at the relatively large open NYC Taxi dataset
(available from the [Taxi & Limousine
Commission](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page)),
and show some numbers, graphs, heatmaps, as well as an animation, that
was created using the Accelerator.

Everything we show is **reproducible**, and one section of this post is
dedicated to how to setup and run the analyses we present on **your own
computer**.




# The Dataset

The dataset is a collection of taxi trips.  There is one file per
month, and each file is in a tabular format.  Each line corresponds to
one taxi trip, and contains information about pickup and dropoff
locations and times, payments, and some more related data.  In total,
there are 1.2 billion (1,190,645,813) lines with taxi trips with GPS
pickup and dropoff coordinates.  The size of the dataset is about
240GB uncompressed.

The following table is part of output from the Accelerator's `dsinfo`
command, and shows column names and min/max values for one of the
files, `yellow_tripdata_2016-06.csv`:

```
Columns:
    dropoff_datetime       datetime  [1996-06-20 16:23:24, 2016-07-01 23:55:31]
    dropoff_latitude       float32   [     0.000000,     60.040714]
    dropoff_longitude      float32   [-118.18625641,  106.24687958]
    fare_amount            float32   [   -450.00000,  628544.75000]
    improvement_surcharge  float32   [ -0.300000012,  11.640000343]
    mta_tax                float32   [ -2.700000048,  60.349998474]
    passenger_count        int32     [            0,             9]
    payment_type           unicode
    pickup_datetime        datetime  [2016-06-01 00:00:00, 2016-06-30 23:59:59]
    pickup_latitude        float32   [     0.000000,     64.096481]
    pickup_longitude       float32   [-118.18625641,    0.00000000]
    rate_code              int32     [            1,            99]
    store_and_fwd_flag     unicode
    surcharge              float32   [ -41.22999954,  597.91998291]
    tip_amount             float32   [ -67.69999695,  854.84997559]
    tolls_amount           float32   [ -12.50000000,  970.00000000]
    total_amount           float32   [   -450.79999,  629033.75000]
    trip_distance          float32   [     0.000000,  71732.703125]
    vendor_id              unicode
```

If we look closely, we can see that there are indeed some strange
values that need further investigation.  In general, datasets
typically comes with a number of issues, of which some are documented,
and some not.  The NYC dataset is of high quality, and we'll have a
look at some of the issues we've found later in this post.




# A First Analysis: Pickup/Dropoff Heatmaps

Using the `pickup` and `dropoff` location data columns, it is
straightforward to plot a heatmap over New York City.  A single
heatmap mixes a lot of information together, so instead we make it
much more interesting by creating a set of plots to examine.  This is
easily done using the Accelerator's powerful build scripts.

First, to see the difference between pickup and dropoff locations, we
plot these separately.  Then, we are curious about the `vendor_id`
column, and want to know if data is somehow different for different
vendors.  Here is the distribution of values in the `vendor_id`
column:

```
  vendor          #rows
       1    102.008.367
       2    113.511.124
     CMT    505.603.754
     DDS     14.169.148
     VTS    513.853.077
```

Data from different vendors is also separated in time, but we've
chosen not to include that analysis here for brevity reasons.



### The Heatmaps

First, let's have a look at the difference between _pickup_ and
_dropoff_ locations.  Below we show these for vendor "1":

<p float="center">
   <img src="{{ site.url }}/assets/coord_1_P.png" height="400"/>
   <img src="{{ site.url }}/assets/coord_1_D.png" height="400"/>
</p>

There is more "detail" in the dropoff data.  A hypothesis is that
dropoff locations correspond to where the customer wants to go, which
could be anywhere, whereas the pickup locations correspond to where
it is easiest to catch a cab.

Below are heatmaps of cab **dropoff** positions around central
Manhattan, one plot for each of the five vendors.  The most common
dropoff locations are close to the Penn Station on the 7th and 8th
Avenue.

<p float="center">
   <img src="{{ site.url }}/assets/coord_1_D.png" height="400"/>
   <img src="{{ site.url }}/assets/coord_2_D.png" height="400"/>
</p>
<p float="center">
   <img src="{{ site.url }}/assets/coord_CMT_D.png" height="400"/>
   <img src="{{ site.url }}/assets/coord_DDS_D.png" height="400"/>
</p>
<p float="center">
   <img src="{{ site.url }}/assets/coord_VTS_D.png" height="400"/>
   <img src="{{ site.url }}/assets/coord_VTS_DAntarctica.png" height="400"/>
</p>

Before jumping to conclusions, we note that the plots above cover
_all_ data for each vendor, and the most noisy graphs corresponds to
the vendors with most data.  However, limiting each graph to only
100.000.000 points, the results are similar.  _Some vendors have
higher quality positional data than others_.  **Remember that vendors
also were distributed in time, so one hypothesis is that this is
partly because of technology improvements.  We have not investigated
this further.**

Some of the plots seem noisy, but actually, it turns out that some of
the noise is not noise at all.  Take a look at the bottom right image
above.  This heatmap shows data from the "VTS" vendor centered around
the position on the _South Pole_ that corresponds to the location of
Empire State Building _if its latitude and longitude coordinates were
swapped!_  Some equipment out there has been travelling with its
coordinates mixed up.  Interesting!  There are probably more issues
like this, see section on data quality below.




# A Second Analysis:  Ongoing Taxi Rides

It would be interesting to know the number of cabs is traffic at each
instance in time.  This information is not explicit in the dataset,
but it can be computed easily.  Here is how:

First, we realise that the resolution of the timestamp data in the
dataset is one second.  There is no need to carry out our analysis
with such a high resolution, it will only lead to more computations
and a noisy result.  It is more than sufficient to work on a minute
resolution.  There are about _four million minutes_ in the dataset
(2009-01-01 to 2016-06-30), which is a reasonable amount of data to
handle without difficulty.

Now, each row in the dataset corresponds to a complete trip.  Each
trip has both starting and ending timestamps.  To compute data for the
number of cabs in traffic each minute we can do as follows

1.  Create a vector of all zeros with a length of four million
    entries.  Each item will correspond to one minute.

2.  For each ride, increase the vector item at the start time offset by one,
    and decrease the vector item at the stop time offset by one.

3.  Integrate, i.e. sum the vector to get the number of ongoing trips
    per minute.

These operations execute fast in linear time, and they do not consume
a lot of memory.

Let's look at the resulting curves.



## Yearly Cycles

First, we look at all the data from 2009 to 2016 in one go.  Plotting
with a minute resolution on this scale is far to noisy to be useful.
In fact, there are thousands of samples for each pixel rendered on the
screen.  So we downsample.  Actually, we do downsampling in two steps.
First, all data is added into bins, where each bin representing one
calendar day.  From there, a 30 day root raised cosine filter is
applied to smooth the curve into a month-like timescale.  The
resulting curve is shown below:

<p align="center"><img src="{{ site.url }}/assets/nyc_rides_yearly.png" height="200"> </p>

There is a clear periodicity, with a period time of one year.  Each
year, there is a strong dip during summer, a narrower dip during
Christmas, and the peak is somewhere around April.



## Daily and Weekly Cycles

In order to make sense of a plot with minute resolution, we have to
zoom in.  This is preferably done interactively with some plotting
tool.  Since the datafile is about 94MB, one has to choose this
program with care.  We use [gnuplot](http://www.gnuplot.info/), and it
handles this file pretty well.

Here is one interesting zoom-in, covering December 2015 to January
2016:

<p align="center"><img src="{{ site.url }}/assets/nyc_rides_201512-201601.png" height="350"> </p>

In this graph we can see the daily and weekly periods.  We can also
see that there is a drop in the number of simultaneous taxi rides over
Christmas.  But what is the drop all the way down to zero that we see
in late January?  Were there no cabs on duty at that time?

We zoom in (and this is where the high resolution pays off):

<p float="center">
   <img src="{{ site.url }}/assets/nyc_rides_snowzilla.png" height="340"/>
   <img src="{{ site.url }}/assets/nyc_wikipedia_snowzilla.jpg" height="340"/>
</p>

A search on the Internet reveals that the dip corresponds in time with
the [January 2016 United States
Blizzard](https://en.wikipedia.org/wiki/January_2016_United_States_blizzard),
where a travel ban was instituted for New York City.  (Image from
Wikipedia.)



## Comments

This was just a brief sample.  There are a lot of interesting things
that can be found if playing with this minute resolution file!

Data relating to human activities typically experience periodicity on
year, week, and day levels.

What about the total number of taxis in New York?  Clearly, from the
graph above, we can se that it happens that more than 8000 cabs are in
duty at the same time!  According to Wikipedia, there are about 15.000
"medallions" for New York City.

Gnuplot has been around and in constant development since 1986.






# A Third Analysis:  Making an Animation

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/SaUyRZPwFYg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>

Animations are sometimes a great way to visualise datasets.  In this
case, we create a video from still frames, where each frame is a
heatmap of pickup and dropoff locations on a map.  We use red-ish
colours for pickups (think red as in urgent to get a cab), and
blue-ish for dropoffs.

Creating all these heatmaps consumes a lot of CPU cycles.  To minimise
execution time, we make sure that generation of the frames is carried
out entirely in parallel.  The vintage
[machine](https://expertmakeraccelerator.org/performance/2019/09/02/bigdata_on_inexpensive_workstation.html)
we are using extracts heatmap data at a rate of about 5000 heatmaps
per second.  Drawing the heatmaps using
[pillow](https://python-pillow.org) is significantly slower, though.
But will full parallelisation, the machine can draw about 125 frames
per second.  At 60Hz framerate, this is twice playback realtime.
Then, [ffpmeg](https://www.ffmpeg.org/) needs about the same time to
encode the frames to h.264 video format.

**In this /post/, we explain in more detail how the animation was
created and how to do simple and efficient parallel processing using
the Accelerator.**







# Try it out Yourself!

Everything we've shown in this post is easily reproducible in a few
minutes processing time.  This section tells you how.


## Install the Accelerator
The Accelerator is installed using `pip`
```
pip install accelerator
```

### Dependencies

To run the scripts that generate _graphics and video_, you also need to
install the [Python Imaging Library](https://python-pillow.org)
(a.k.a. *PIL* or *pillow*), [gnuplot](http://www.gnuplot.info/), and
[ffmpeg](https://www.ffmpeg.org/).  Install pillow for example using
`pip install pillow`.  In a debian-like distribution, gnuplot and
ffmpeg can be installed using `apt-get install gnuplot ffmpeg`.



## Clone the Project

The project is available on *github*.  Do

```
git clone http://github.com/berkeman/nyc
```

followed by

```
cd nyc
```

Everything should be set up, but please have a look at the
`accelerator.conf` configuration file.  Adjust the `slices` parameter
to match the number of cores in the machine.  If you want to store
data and jobs elsewhere, please modify the definition of the `dev`
workdir and the `input directory`.



## Download the Dataset

The dataset is available from the [TLC Trip Record
Data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page)
page.  The files we've used are the `Yellow Taxi Trip Records` ones.
For now, download this one

[https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-01.csv](https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-01.csv)

for example like this
```bash
cd input_directory
wget https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-01.csv
```

Note that the data must be stored in the `input directory` as defined
by the configuration file.



## Start the Accelerator Server

The Accelerator is a client-server application.  Start the server by issuing

```bash
ax server
```

in the `nyc` project directory.  (Use `ax server --help` for more information.)



## Run the Build Script

For simplicity, there is a single build script that will import and
process a single datafile.  The script is
`nyc/build_simple_verbose.py` and it is executed like this

```bash
ax run simple_verbose
```

Execute this command in the `nyc` directory with the server running.

This build script will import, type, and process the input file.  It
will create a few heatmaps and graphs, as well as a movie file.
Output files will be stored in the `result` directory.


(There is a corresponding `build_simple` doing the same operations, but
without all the printouts.)





# Dataset Issues

The NYC Taxi dataset is of high quality.  But all datasets comes with
issues of various kinds.  A good thing with this dataset is that there
is an _errata_ with known problems and additional information.

In this chapter, we present a few things that impact the _import_ of
the dataset.



## Column Names

Each data file starts with a header containing column names for all
columns in the file.  Unfortunately, *both names and formatting change
between files*.  Some headers use space+comma as the field separator,
while some use just comma.  In addition, for some, but not all, files,
there is an additional empty line between the header and the data.

The solution to these problems is composed of two parts.  First, we
write a function that reads the first lines in the datafile to
determine the column names and the number of blank lines present
before the actual data.  Second, we create a simple dictionary that
maps all label names to a canonical set of names that is used for all
files throughout the dataset.



## Bad Lines

Two of the files in the dataset contain a total of 6809 "unparseable"
lines.  The files are

```
yellow_tripdata_2010-02.csv      # 1126 bad lines
yellow_tripdata_2010-03.csv      # 5683 bad lines
```

All these lines share the property that they contain one more column
than all other lines.

The Accelerator finds and isolates these lines automatically.  This is
the way it works: On a first data import, the import program will
crash with an error, since parsing is by default assumed to be
_strict_.  Thus, the programmer is informed there is a problem.  The
next step is to re-run the import for the file causing the crash with
the `allow_bad` flag set.  When the `allow_bad` flag is set, all
unparseable lines are stored in a separate dataset named `bad`, one
for each imported file, and when the import of all files is complete
we can check the contents of these datasets.  The simplest way to
examine these dataset is using the `dsinfo` tool, that actually comes
with an option to show only those datasets that contain one line or
more of data.  So we run

```
% ax dsinfo nyc-179/bad -C
```

and get

```
    ...
    Full chain length 90, from nyc-0/bad to nyc-178/bad
    Filtered chain length 2
          0: nyc-26/bad (1,126)
          1: nyc-28/bad (5,683)
    6,809 total lines in chain
```

By doing `dsinfo` lookups on the two datasets, we see that they
correspond to the files mentioned above.  Since the datasets are
small, we can investigate them further using the 'dsgrep' tool and
some simple shell commands, for example like this

```
% ax dsgrep . nyc-26/bad data | awk -F, '{print NF}' | uniq
```

The command above will grep anything (a dot regexp) in the `data`
column of the `nyc-26/bad` dataset, and paste it to an `awk` program
that will print the number of fields separated by a comma.  This data
is then fed to `uniq` that will print unique lines only once.  The
output of the above program is

```
19
```

while

```
% ax dsinfo nyc-26/default
```

tells us that there are 18 columns in the dataset:

```
    Filename: /zbd/data/nyctaxi2019/yellow_tripdata_2010-02.csv
    ...
    18 columns
    ...
```

The same goes for the dataset `nyc-28/bad`.  Thus, these lines caused
the import to fail because they had an unexpected extra column.



## Outliers

For the complete dataset, min/max values look like this

```
% ax dsinfo nyc-179

Columns:                                 min value       max value
       dropoff_datetime       datetime  [1900-01-01 00:00:00, 2253-08-23 07:56:38]
       dropoff_latitude       float32   [-3579.1394043,  3577.1350098]
       dropoff_longitude      float32   [-3579.1394043,  3460.4267578]
       fare_amount            float32   [-21474808.000,    825998.625]
       improvement_surcharge  float32   [  -0.30000001,  137.63000488]
       mta_tax                float32   [   -3.0000000,  1311.2199707]
       passenger_count        int32     [            0,           255]
       payment_type           unicode   
       pickup_datetime        datetime  [2009-01-01 00:00:00, 2016-06-30 23:59:59]
       pickup_latitude        float32   [-3579.1394043,  3577.1354980]
       pickup_longitude       float32   [-3509.0151367,  3570.2241211]
       rate_code              int32     [            0,           252]
       store_and_fwd_flag     unicode   
       surcharge              float32   [ -79.00000000,  999.98999023]
       tip_amount             float32   [-1677720.1250,  3950588.7500]
       tolls_amount           float32   [-21474836.000,      5510.070]
       total_amount           float32   [-21474830.000,   3950611.500]
       trip_distance          float32   [ -40840124.00,  198623008.00]
       vendor_id              unicode   
```

Clearly, most min and max values does not make any sense: 255
passengers?  3460 degrees (i.e. ten turns around the earth)?
$3950611.5 for a taxi ride?  One of the columns seems to be perfectly
fine, though, and that is the `pickup_datetime` column.  Its range is
exactly as expected.  A bit strange.

A deeper analysis is needed to understand this, of course depending on
the application.  As we've seen there are also correlations between
"vendors" and specific data problems, so not all problems we see are
general.



# Execution Times


# Conclusion

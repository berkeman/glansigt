---
layout: post
title:  "Fast Processing of the NYC Yellow Cab Dataset"
date:   2020-01-02 00:00:00
categories: example
---

<p align="center"><img src="{{ site.url }}/assets/nyc_heatmap.png" height="600"> </p>




# Introduction

The Accelerator from eBay is the perfect tool for processing of large
datasets on a single machine.  In this post, we look at the relatively
large open NYC Taxi dataset (available from the [Taxi & Limousine
Commission](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page)),
and show how to efficiently process it to compute some numbers,
graphs, heatmaps, and animations.




# The Dataset

The dataset is a collection of taxi trips.  There is one file per
month, and one taxi trip per line.  Each line contains information
such as pickup and dropoff position and time, payments, and more.  In
total, there are 1.2 billion (1,190,645,813) lines with taxi trips
with GPS pickup and dropoff coordinates.  The size of the dataset is
about 240GB uncompressed.

To get a first feel for it, here are the column names and min/max
values for one of the files, `yellow_tripdata_2016-06.csv`:

```
Columns:
    dropoff_datetime       datetime  [1996-06-20 16:23:24, 2016-07-01 23:55:31]
    dropoff_latitude       float32   [     0.000000,     60.040714]
    dropoff_longitude      float32   [-118.18625641,  106.24687958]
    fare_amount            float32   [   -450.00000,  628544.75000]
    improvement_surcharge  float32   [ -0.300000012,  11.640000343]
    mta_tax                float32   [ -2.700000048,  60.349998474]
    passenger_count        int32     [            0,             9]
    payment_type           unicode
    pickup_datetime        datetime  [2016-06-01 00:00:00, 2016-06-30 23:59:59]
    pickup_latitude        float32   [     0.000000,     64.096481]
    pickup_longitude       float32   [-118.18625641,    0.00000000]
    rate_code              int32     [            1,            99]
    store_and_fwd_flag     unicode
    surcharge              float32   [ -41.22999954,  597.91998291]
    tip_amount             float32   [ -67.69999695,  854.84997559]
    tolls_amount           float32   [ -12.50000000,  970.00000000]
    total_amount           float32   [   -450.79999,  629033.75000]
    trip_distance          float32   [     0.000000,  71732.703125]
    vendor_id              unicode
```

If we look closely, we can see that there are indeed some strange
values that need further investigation.  In general, datasets
typically comes with a number of issues, of which some are documented,
and some not.  The NYC dataset is of high quality, and we'll have a
look at some of the issues we've found later in this post.




# A First Analysis: Creating Heatmaps of Pickup/Dropoff Locations

Using the `pickup` and `dropoff` location data columns, it is
straightforward to plot a heatmap over New York City.  We can start by
generating one heatmap of all the data, but to make it much more
interesting we create a set of plots to look at.  First, to see the
difference between pickup and dropoff locations, we plot these
separately.  Then, we are curious about the `vendor_id` column, and if
data is different for different vendors.  Here is the distribution of
values in the `vendor_id` column:

```
  vendor        #rows
       1    102.008.367
       2    113.511.124
     CMT    505.603.754
     DDS     14.169.148
     VTS    513.853.077
```

Data from different vendors is also separated in time, but we've
chosen not to include that analysis for brewity reasons.

### The Heatmaps

First, let's have a look at the difference between /pickup/ and
/dropoff/ locations.  Below we show these for vendor "1":

<p float="center">
   <img src="{{ site.url }}/assets/coord_1_P.png" height="400"/>
   <img src="{{ site.url }}/assets/coord_2_D.png" height="400"/>
</p>

There is more "detail" in the dropoff data.  A hypothesis is that
dropoff locations corresponds to where the customer wants to go,
whereas the pickup locations corresponds to where it is easiest to get
a cab.

Below are heatmaps of cab **dropoff** positions around central
Manhattan, one plot per vendor.  The most common dropoff locations are
the the Penn Station exits on 7th and 8th Avenue.

<p float="center">
   <img src="{{ site.url }}/assets/coord_1_D.png" height="400"/>
   <img src="{{ site.url }}/assets/coord_2_D.png" height="400"/>
</p>
<p float="center">
   <img src="{{ site.url }}/assets/coord_CMT_D.png" height="400"/>
   <img src="{{ site.url }}/assets/coord_DDS_D.png" height="400"/>
</p>
<p float="center">
   <img src="{{ site.url }}/assets/coord_VTS_D.png" height="400"/>
   <img src="{{ site.url }}/assets/coord_VTSflip_D.png" height="400"/>
</p>

Before jumping to conclusions, we note that the plots cover _all_ data
for each vendor, and the most noisy graphs corresponds to the vendors
with most data.  However, limiting each graph to only 100.000.000
points, the results are similar.  Some vendor data is better than
other.  Remember that vendors also were distributed in time, so one
hypothesis is that this is partly because of technology
improvements.  We have not investigated this further.

In either case, not everything that appears to be noise is actually
noise.  Have a look at the bottom right image.  This heatmap shows
data from the "VTS" vendor plotted at the position on the _South Pole_
that corresponds to the location of Empire State Building if its
latitude and longitude coordinates were swapped!  Some equipment out
there has been travelling with its coordinates mixed up.  Interesting!
There are probably more issues like this, see section on data quality
below.






### A Second Analysis:  Number of Ongoing Taxi Rides per Time Instance

Each row in the dataset corresponds to a complete trip.  A row has
both starting and ending timestamps.  The time resolution is one
second.  To compute the number of ongoing trips, it is more than
sufficient to work on a minute resolution.  There are about four
million minutes in the dataset (2009-01-01 to 2016-06-30), which is a
reasonable amount of data to handle without difficulty.

If we want to compute a graph of the number of ongoing trips per
minute, we can proceed as follows:

1.  Create a vector of all zeros with a length four million.  Each
    item corresponds to one minute

2.  For each ride, increase the vector item at the start time offset by one,
    and decrease the vector item at the stop time offset by one

3.  Integrate the vector to get the number of ongoing trips per minute

Let's look at the resulting curves.


#### Yearly Cycles

We start with a higher view of all the data from 2009 to 2016.
Plotting with a minute resolution is far to noisy to be useful when
looking at this scale.  In fact, there are thousands of samples for
each pixel on screen.  So we downsample.  Actually, we do downsampling
in two steps.  First, all data is added into bins, where each bin
representing one calendar day.  From there, a 30 day root raised
cosine filter is applied to smooth the curve into a month-like
timescale.  The resulting curve is shown below:

<p align="center"><img src="{{ site.url }}/assets/nyc_rides_yearly.png" height="200"> </p>

There is a clear periodicity, with a period time of one year.  For
example, there is a strong dip during the summer, a narrower one
during Christmas, and the peak is somewhere around April.


#### Daily and Weekly Cycles

In order to make sense of a plot with minute resolution, we have to
zoom in.  This is preferably done interactively with some plotting
tool.  Since the datafile is about 94MB, one has to choose this
program with care.  We use [gnuplot](http://www.gnuplot.info/), and it
handles this file pretty well.

Here is one interesting zoom-in, covering December 2015 and January
2016:

<p align="center"><img src="{{ site.url }}/assets/nyc_rides_201512-201601.png" height="350"> </p>

Here we can see the daily and weekly periods.  We can also see that
there is a drop in the number of simultaneous taxi rides over
Christmas.  But what is the drop all the way down to zero that we see
in late January?  Were there no cabs on duty at that time?

We zoom in:

<p float="center">
   <img src="{{ site.url }}/assets/nyc_rides_snowzilla.png" height="340"/>
   <img src="{{ site.url }}/assets/nyc_wikipedia_snowzilla.jpg" height="340"/>
</p>

A search on the Internet reveals that the dip corresponds in time with
the [January 2016 United States
Blizzard](https://en.wikipedia.org/wiki/January_2016_United_States_blizzard),
where a travel ban was instituted for New York City.  (Image from
Wikipedia.)


#### Comments

Data relating to human activities typically experience periodicity on
year, week, and day levels.  There is a lot of interesting things to
note if playing with the minute resolution file.

According to wikipedia, there are about 15000 "medallions" for New
York City.  Clearly, from the graph above, we can se that it happens
that more than 8000 cabs are in duty at the same time!

Gnuplot has been around and in constant development since 1986.






### A Third Analysis:  Making an Animation

Animations are sometimes a great way to visualise datasets.  In this
case, we create a video from frames calculated as heatmaps of pickup
and dropoff locations on a map.  We use red-ish colours for pickups
(think, red as in urgent to get a cab), and blue-ish for dropoffs.

We make sure that generation of the frames is carried out entirely in
parallel in order to minimise execution time.  The vintage /machine/
we are using extracts heatmap data at a rate of about 5000 heatmaps
per second.  Drawing the heatmaps using /pillow/ is significantly
slower, though.  But will full parallelisation, the machine can draw
about 125 frames per second.  At 60Hz framerate, this is twice
realtime.  Then, /ffpmeg/ needs about the same time to encode the
frames to h.264 video format.

In this /post/, we explain in more detail how the animation was
created and how to do simple and efficient parallel processing using
the Accelerator.





### Now, Try it out Yourself!

#### Install the Accelerator
The Accelerator is installed using `pip`
```
pip install accelerator
```

To run the scripts, you also need to install the *Python Imaging
Library* (a.k.a. *PIL* or *pillow*) and *gnuplot*.  Install pillow for
example using `pip install pillow`.  In a debian-like distribution,
gnuplot can be installed using `apt-get install gnuplot`.



#### Clone the Project

The project is available on *github*.  Do

```
git clone http://github.com/berkeman/nyc
```

followed by

```
cd nyc
```

Everything should be set up, but please have a look at the
`accelerator.conf` configuration file.  Adjust the `slices` parameter
to match the number of cores in the machine.  If you want to store
data and jobs elsewhere, please modify the definition of the `dev`
workdir and the `input directory`.



#### Download the dataset

The dataset is available from the [TLC Trip Record
Data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page)
page.  The files we've used are the `Yellow Taxi Trip Records` ones.
For now, download this one

[https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-01.csv](https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-01.csv)

for example like this
```bash
cd input_directory
wget https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-01.csv
```


#### Start the Accelerator Server

The Accelerator is a client-server application.  Start the server by issuing

```bash
ax server
```

in the `nyc` project directory.  (Use `ax server --help` for more information.)



#### Run the Build Script

For simplicity, there is a single build script that will import and
process a single datafile.  The script is
`nyc/build_simple_verbose.py` and it is executed like this

```bash
ax run simple_verbose
```

Execute this command in the `nyc` directory with the server running.

This build script will import, type, and process the input file.  It
will create a few heatmaps and graphs, as well as a movie file.
Output files will be stored in the `result` directory.


(There is a corresponding `build_simple` doing the same operations, but
without all the printouts.)





### Dataset Issues


All datasets comes with issues of various kinds.  This dataset is of
high quality, but there are some things...


#### Column names

Each data file has a header with column names for all columns in the
file.  Unfortunately, *both names and formatting change between files*.
Some headers use space+comma as the field separator, while some use
just comma.  All data is comma separated, though.  In addition, in
some files there is an empty line between the header and the data.

The solution to these problems is composed of two parts.  First, we
write a function that reads the first lines in the datafile to
determine the labels and how many blank lines there are before the
actual data.  Second, we create a simple dictionary that maps all
label names to a canonical set of names that is used throughout the
dataset.


#### Bad lines

Two of the files in the dataset contain a total of 6809 "unparseable"
lines.  The files are

```
yellow_tripdata_2010-02.csv      # 1126 bad lines
yellow_tripdata_2010-03.csv      # 5683 bad lines
```

All these lines share the property that they contain one more column
than all other lines.

The Accelerator finds and isolates these lines automatically.  This is
the way it works.  On a first data import, the import program will
crash, since parsing is by default assumed to be strict.  Thus, the
programmer is informed there is a problem.  The next step is to re-run
the import at the position of the crash with the `allow_bad` flag set.
When the `allow_bad` flag is set, all unparseable lines are stored in
a separate dataset named `bad`, one for each input file, and after the
full import completes we can check the contents of these datasets.
The simplest way to examine these dataset is using the `dsinfo` tool,
that actually comes with an option to show only those datasets that
contain one line or more of data.  So we run
```
% ax dsinfo nyc-179/bad -C
```
and get
```
    ...
    Full chain length 90, from nyc-0/bad to nyc-178/bad
    Filtered chain length 2
          0: nyc-26/bad (1,126)
          1: nyc-28/bad (5,683)
    6,809 total lines in chain
```

By doing `dsinfo` lookups on the two datasets, we see that they
correspond to the files mentioned above.  Since the datasets are
small, we can investigate them further will simple shell commands, for
example like this

```
% ax dsgrep . nyc-26/bad data | awk -F, '{print NF}' | uniq
```

The command above will grep anything (a dot regexp) in the `data`
column of the `nyc-26/bad` dataset, and paste it to an `awk` program
that will print the number of fields separated by a comma.  This data
is then fed to `uniq` that will print unique lines only once.  The
output of the above program is

```
19
```

while

```
% ax dsinfo nyc-26/default
```

tells us that there are 18 columns in the dataset:
```
    Filename: /zbd/data/nyctaxi2019/yellow_tripdata_2010-02.csv
    ...
    18 columns
    ...
```
The same goes for the dataset `nyc-28/bad`.



#### Outliers


For the complete dataset, min/max values look like this

```
% ax dsinfo nyc-179

Columns:                                 min value       max value
       dropoff_datetime       datetime  [1900-01-01 00:00:00, 2253-08-23 07:56:38]
       dropoff_latitude       float32   [-3579.1394043,  3577.1350098]
       dropoff_longitude      float32   [-3579.1394043,  3460.4267578]
       fare_amount            float32   [-21474808.000,    825998.625]
       improvement_surcharge  float32   [  -0.30000001,  137.63000488]
       mta_tax                float32   [   -3.0000000,  1311.2199707]
       passenger_count        int32     [            0,           255]
       payment_type           unicode   
       pickup_datetime        datetime  [2009-01-01 00:00:00, 2016-06-30 23:59:59]
       pickup_latitude        float32   [-3579.1394043,  3577.1354980]
       pickup_longitude       float32   [-3509.0151367,  3570.2241211]
       rate_code              int32     [            0,           252]
       store_and_fwd_flag     unicode   
       surcharge              float32   [ -79.00000000,  999.98999023]
       tip_amount             float32   [-1677720.1250,  3950588.7500]
       tolls_amount           float32   [-21474836.000,      5510.070]
       total_amount           float32   [-21474830.000,   3950611.500]
       trip_distance          float32   [ -40840124.00,  198623008.00]
       vendor_id              unicode   
```

Clearly, most min and max values does not make any sense: 255
passengers?  3460 degrees (i.e. ten turns around the earth)?
$3950611.5 for a taxi ride?  One of the columns seems to be perfectly
fine, though, and that is the `pickup_datetime` column.  Its range is
exactly as expected.  A bit strange.

A deeper analysis is needed to understand this, of course depending on
the application.  As we've seen there are also correlations between
"vendors" and specific data problems, so not all problems are general
problems.

